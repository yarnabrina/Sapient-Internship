{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FR - EN Vanilla Encoder Decoder Machine Translation Model with LSTM.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TkCOviiKFRtc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# importing required modules\n",
        "import matplotlib.pyplot as plt, re\n",
        "from google.colab import files\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Embedding, LSTM, RepeatVector, TimeDistributed\n",
        "from keras.models import load_model, Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from numpy import argmax, array, empty, mean\n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from unicodedata import category, normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L9wzBF0FQ-e_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# removing the accents\n",
        "def unicode_to_ascii(string):\n",
        "    return ''.join(character for character in normalize('NFD', string) if category(character) != 'Mn')\n",
        "\n",
        "# cleaning a sentence\n",
        "def preprocess_sentence(line):\n",
        "    # converting from unicode to ascii\n",
        "    line = unicode_to_ascii(line.lower().strip())\n",
        "    # creating a space between a word and the punctuation following it, and then collapsing multiple spaces\n",
        "    line = re.sub(r'[\" \"]+', \" \", re.sub(r\"([!',-.0-9?])\", r\" \\1 \", line))\n",
        "    # replacing everything with space except a-z, A-Z, \"!\", \"'\", \",\", \"-\", \".\", \"?\"\n",
        "    line = re.sub(r\"[^a-zA-Z!',-.0-9?]+\", \" \", line).strip()\n",
        "    return line\n",
        "\n",
        "# creating line pairs in the format: [target, source]\n",
        "def create_dataset(filename, size):\n",
        "    # open the file as read only, read all text and split in lines\n",
        "    pairs = open(filename, mode = 'rt', encoding='UTF-8').read().strip().split('\\n')\n",
        "    # reducing dataset, splitting into target - source pairs and cleaning\n",
        "    line_pairs = [[preprocess_sentence(line) for line in pair.split('\\t')] for pair in pairs[:size]]\n",
        "    return array(line_pairs)\n",
        "\n",
        "# finding maximum sentence length\n",
        "def maximum_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "# creating a tokenizer to vectorize the sequence\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer(filters = '\"#$%&()*+/:;<=>@[\\]^_`{|}~')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# encoding and padding the sequence to the maximum length\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # encoding sequence to integers\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # padding sequence with 0 values\n",
        "    X = pad_sequences(X, maxlen = length, padding = 'post')\n",
        "    return X\n",
        "\n",
        "# performing one hot encode on target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "    ylist = []\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes = vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y\n",
        "\n",
        "# loading training and testing data\n",
        "def load_dataset(filename, size, shuffle_state = True, test_proportion = 0.2):\n",
        "    # loading reduced modified line pairs\n",
        "    combined_dataset = create_dataset(filename, size)\n",
        "    # shuffling and splitting into training and testing subsets\n",
        "    training_dataset, testing_dataset = train_test_split(combined_dataset, shuffle = shuffle_state, test_size = test_proportion)\n",
        "    # preparing target tokenizer\n",
        "    target_tokenizer = create_tokenizer(combined_dataset[:, 0])\n",
        "    target_vocabulary_size = len(target_tokenizer.word_index) + 1\n",
        "    maximum_target_length = maximum_length(combined_dataset[:, 0])\n",
        "    # preparing source tokenizer\n",
        "    source_tokenizer = create_tokenizer(combined_dataset[:, 1])\n",
        "    source_vocabulary_size = len(source_tokenizer.word_index) + 1\n",
        "    maximum_source_length = maximum_length(combined_dataset[:, 1])\n",
        "    # preparing training data\n",
        "    training_source = encode_sequences(source_tokenizer, maximum_source_length, training_dataset[:, 1])\n",
        "    training_target = encode_sequences(target_tokenizer, maximum_target_length, training_dataset[:, 0])\n",
        "    training_target = encode_output(training_target, target_vocabulary_size)\n",
        "    # preparing testing data\n",
        "    testing_source = encode_sequences(source_tokenizer, maximum_source_length, testing_dataset[:, 1])\n",
        "    testing_target = encode_sequences(target_tokenizer, maximum_target_length, testing_dataset[:, 0])\n",
        "    testing_target = encode_output(testing_target, target_vocabulary_size)\n",
        "    # printing dataset information\n",
        "    print('Source Vocabulary Size: %d' % source_vocabulary_size)\n",
        "    print('Source Maximum Length: %d' % maximum_source_length)\n",
        "    print('Target Vocabulary Size: %d' % target_vocabulary_size)\n",
        "    print('Target Maximum Length: %d' % maximum_target_length)\n",
        "    return combined_dataset, training_dataset, testing_dataset, training_source, training_target, testing_source, testing_target, source_vocabulary_size, target_vocabulary_size, maximum_source_length, maximum_target_length, source_tokenizer, target_tokenizer\n",
        "\n",
        "# defining encoder decoder neural machine translation model\n",
        "def define_model(source_vocabulary, target_vocabulary, source_timesteps, target_timesteps, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(source_vocabulary, n_units, input_length = source_timesteps, mask_zero = True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(target_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences = True))\n",
        "    model.add(TimeDistributed(Dense(target_vocabulary, activation = 'softmax')))\n",
        "    return model\n",
        "\n",
        "# mapping an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# generating target sequence given source sequence\n",
        "def predict_sequence(model, target_tokenizer, source):\n",
        "    prediction = model.predict(source, verbose = 0)[0]\n",
        "    integers = [argmax(vector) for vector in prediction]\n",
        "    target = []\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, target_tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)\n",
        "    \n",
        "# evaluating a fitted model\n",
        "def evaluate_model(model, target_tokenizer, testing_source, testing_target, testing_dataset):\n",
        "    cosine_similarities = empty(len(testing_target))\n",
        "    for i, source in enumerate(testing_source):\n",
        "        # decoding predicted translations of encoded source text\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        prediction = predict_sequence(model, target_tokenizer, source)\n",
        "        target = testing_dataset[i][0]\n",
        "        # calculating cosine similarity\n",
        "        vectorisations = TfidfVectorizer().fit_transform([prediction, target])\n",
        "        cosine_similarities[i] = cosine_similarity(vectorisations[0,], vectorisations[1,])\n",
        "    # checking model performance\n",
        "    metrics = model.evaluate(testing_source, testing_target, verbose = 0)\n",
        "    # printing results\n",
        "    print(\"Average Cosine Similarity: %.2f\" % mean(cosine_similarities))\n",
        "    print(\"Model %s: %.2f%%\" % (model.metrics_names[1], metrics[1] * 100))\n",
        "\n",
        "# translating source language text to target language text\n",
        "def translate(source, model, source_tokenizer, target_tokenizer, maximum_source_length):\n",
        "    encoded_source = encode_sequences(source_tokenizer, maximum_source_length, array([preprocess_sentence(source.strip())]))[0]\n",
        "    decoded_translation = predict_sequence(model, target_tokenizer, encoded_source.reshape((1, encoded_source.shape[0])))\n",
        "    return(decoded_translation)\n",
        "\n",
        "# visualising a fitted model\n",
        "def visualise_model(model, history, name, combined_dataset, source_tokenizer, target_tokenizer, maximum_source_length):\n",
        "    print(model.summary())\n",
        "    # plotting progress\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_categorical_accuracy'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Improvement during Training')\n",
        "    plt.ylabel('Model Metrics')\n",
        "    plt.xlabel('Number of Epochs Passed')\n",
        "    plt.legend(['training accuracy', 'training loss', 'validation accuracy', 'validation loss'], loc = 'best')\n",
        "    plt.savefig('%s_progress.png' % name)\n",
        "    plt.close()\n",
        "    for counter, language_pair in enumerate(combined_dataset):\n",
        "        if counter % 2500 == 0:\n",
        "            input_text = language_pair[1]\n",
        "            expected_text = language_pair[0]\n",
        "            output_text = translate(language_pair[1], model, source_tokenizer, target_tokenizer, maximum_source_length)\n",
        "            print('source: [%s] \\t target: [%s] \\t result: [%s]' % (input_text, expected_text, output_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E_UhEUFiiSQu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d82d1c52-2d81-4c55-8f2b-d30650644400",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530601293172,
          "user_tz": -330,
          "elapsed": 152345,
          "user": {
            "displayName": "Anirban Ray",
            "photoUrl": "//lh6.googleusercontent.com/-YWeRHysVjQ4/AAAAAAAAAAI/AAAAAAAAFzA/RWbskWMuEvY/s50-c-k-no/photo.jpg",
            "userId": "102106891618891288161"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# removing all existing files\n",
        "files_in_directory = [file for file in listdir('.') if isfile(join('.', file))]\n",
        "for counter in files_in_directory:\n",
        "    remove(counter)\n",
        "\n",
        "# uploading file\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e64409aa-dec6-44a4-8774-4865d36b0752\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e64409aa-dec6-44a4-8774-4865d36b0752\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving fra.txt to fra.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIkEEPpBtWFq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1904
        },
        "outputId": "49f4950e-129b-4888-e57d-40136ba95a54",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530602913306,
          "user_tz": -330,
          "elapsed": 1620124,
          "user": {
            "displayName": "Anirban Ray",
            "photoUrl": "//lh6.googleusercontent.com/-YWeRHysVjQ4/AAAAAAAAAAI/AAAAAAAAFzA/RWbskWMuEvY/s50-c-k-no/photo.jpg",
            "userId": "102106891618891288161"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# loading reduced dataset\n",
        "n_sentences = 35000\n",
        "combined, train, test, trainX, trainY, testX, testY, vocabX, vocabY, sizeX, sizeY, tokenX, tokenY = load_dataset('fra.txt', n_sentences)\n",
        "\n",
        "# defining model\n",
        "fr_en_ed_model = define_model(vocabX, vocabY, sizeX, sizeY, 256)\n",
        "fr_en_ed_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
        "\n",
        "# setting callbacks to for the model during training\n",
        "checkpoint = ModelCheckpoint(filepath = 'fr_en_ed_model.h5', save_best_only = True, verbose = 1)\n",
        "earlystop = EarlyStopping(min_delta = 0.001, patience = 5, verbose = 1)\n",
        "\n",
        "# fitting and visualising and evaluating the model\n",
        "fr_en_ed_progress = fr_en_ed_model.fit(trainX, trainY, epochs = 100, batch_size = 64, validation_split = 0.25, callbacks = [checkpoint, earlystop], verbose = 0)\n",
        "fr_en_ed_model = load_model('fr_en_ed_model.h5')\n",
        "evaluate_model(fr_en_ed_model, tokenY, testX, testY, test)\n",
        "visualise_model(fr_en_ed_model, fr_en_ed_progress, 'fr_en_ed', combined, tokenX, tokenY, sizeX)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source Vocabulary Size: 8372\n",
            "Source Maximum Length: 18\n",
            "Target Vocabulary Size: 4837\n",
            "Target Maximum Length: 11\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.49202, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.49202 to 2.36636, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.36636 to 2.29507, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.29507 to 2.12608, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.12608 to 2.04446, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.04446 to 1.92310, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.92310 to 1.83909, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.83909 to 1.76335, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.76335 to 1.71073, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.71073 to 1.66829, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.66829 to 1.60448, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.60448 to 1.56850, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.56850 to 1.53628, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.53628 to 1.50415, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.50415 to 1.46620, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.46620 to 1.43876, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.43876 to 1.41949, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.41949 to 1.39800, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.39800 to 1.37741, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.37741 to 1.35248, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.35248 to 1.34008, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.34008 to 1.33339, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.33339 to 1.31523, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.31523 to 1.30326, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.30326 to 1.29798, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.29798 to 1.28158, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.28158\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.28158\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.28158\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.28158\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.28158 to 1.27681, saving model to fr_en_ed_model.h5\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.27681\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.27681\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.27681\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.27681\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.27681\n",
            "Epoch 00036: early stopping\n",
            "Average Cosine Similarity: 0.46\n",
            "Model categorical_accuracy: 77.95%\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 18, 256)           2143232   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 11, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 11, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 11, 4837)          1243109   \n",
            "=================================================================\n",
            "Total params: 4,436,965\n",
            "Trainable params: 4,436,965\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "source: [va !] \t target: [go .] \t result: [go .]\n",
            "source: [remets - le en place !] \t target: [put it back .] \t result: [put it back .]\n",
            "source: [j ' ai fini par l ' emporter .] \t target: [i finally won .] \t result: [i finally winning .]\n",
            "source: [j ' ai vu le match .] \t target: [i saw the game .] \t result: [i saw the yesterday .]\n",
            "source: [il me faut me battre .] \t target: [i have to fight .] \t result: [i have to fight .]\n",
            "source: [telephonez au 1 1 0 immediatement .] \t target: [dial 1 1 0 at once .] \t result: [tom used to to 0 . .]\n",
            "source: [ils vont faire des emplettes .] \t target: [they go shopping .] \t result: [they go shopping .]\n",
            "source: [je vous ai laisse une note .] \t target: [i left you a note .] \t result: [i saw you a note .]\n",
            "source: [qui a fait cette tarte ?] \t target: [who made this pie ?] \t result: [who made this pie ?]\n",
            "source: [je fais de mon mieux .] \t target: [i ' m trying my best .] \t result: [i ' m best best . .]\n",
            "source: [tu es toujours vivante .] \t target: [you ' re still alive .] \t result: [you ' re still alive .]\n",
            "source: [ca peut etre difficile .] \t target: [it can be difficult .] \t result: [it may be difficult .]\n",
            "source: [vous etes fort effronte .] \t target: [you ' re very forward .] \t result: [you ' re very forward .]\n",
            "source: [je vais le garder avec moi .] \t target: [i ' ll keep it with me .] \t result: [i ' ll to to him .]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CrgQC6Pbsre6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# downloading files\n",
        "files.download('fr_en_ed_progress.png')\n",
        "files.download('fr_en_ed_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}