{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Indices:  [101, 102, 14, 17, 16, 70, 1, 10, 42, 47, 7, 100, 87, 69, 57, 35, 6, 0, 67, 54, 81, 91, 64, 31, 84, 13, 23, 63, 98, 26]\n",
      "\n",
      "Average Errors:  [0.3878 0.1916 0.6221 0.4842]\n",
      "\n",
      "Decisions:\t ['Reject', 'Reject', 'Reject', 'Reject']\n",
      "\n",
      "Random Indices:  [826, 124, 804, 226, 342, 344, 795, 765, 990, 582, 515, 256, 432, 117, 557, 502, 507, 406, 657, 908]\n",
      "\n",
      "Average Errors: \n",
      " [0.28063306 0.71946963 0.66862643 0.79289759 0.80104164 0.71013695\n",
      " 0.12611055 0.69480752 0.62038353 0.72771856 0.72865124 0.70723496\n",
      " 0.48614193 0.87865842 0.80640339 0.89505037 0.92556319 0.82620421\n",
      " 0.32446107 0.80368939 0.79274617 0.87794534 0.87112637 0.82986264]\n",
      "\n",
      "Decisions:\t ['Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject']\n",
      "\n",
      "Random Indices:  [840653, 1617751, 1650128, 791866, 1531337, 1450964, 190561, 2003370, 62400, 533374, 1541686, 1885724, 1287676, 1644974, 180369, 1004693, 514146, 1677371, 987664, 920034, 850991, 1831325, 57416, 1698972, 232139, 105815, 445507, 1615611, 28106, 116605, 1343398, 1826526, 1605831, 1040347, 1844020, 1844527, 1516630, 512590, 1170164, 926270, 896067, 1305994, 965599, 1520794, 817080, 1611425, 50778, 611651, 1194410, 1639264]\n",
      "\n",
      "Average Cosine Similarities:  [0.4338 0.4164]\n",
      "\n",
      "Decisions:\t ['Reject', 'Reject']\n"
     ]
    }
   ],
   "source": [
    "# Importing the required packages\n",
    "import numpy as np, pandas as pd, random, re, speech_recognition as sr, string, time\n",
    "from googletrans import Translator\n",
    "from pickle import dump\n",
    "from pydub import AudioSegment as AS\n",
    "from scipy.stats import t\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from unicodedata import normalize\n",
    "from yandex_translate import YandexTranslate\n",
    "\n",
    "# Speech Recogition <=> speech to text\n",
    "# Defining a function to ignore everything but letters and space\n",
    "def compromise(text_to_be_modified):\n",
    "    return(re.sub('[^a-zA-Z\" \"]', '', text_to_be_modified))\n",
    "\n",
    "# Defining a function to compute Levenshtein Distance\n",
    "def levenshtein(source, target):\n",
    "    if len(source) < len(target):\n",
    "        return(levenshtein(target, source))\n",
    "    if len(source) == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        if len(target) == 0:\n",
    "            return(1)\n",
    "        source = np.array(tuple(source))\n",
    "        target = np.array(tuple(target))\n",
    "        previous_row = np.arange(target.size + 1)\n",
    "        for s in source:\n",
    "            current_row = previous_row + 1\n",
    "            current_row[1:] = np.minimum(current_row[1:], np.add(previous_row[:-1], target != s))\n",
    "            current_row[1:] = np.minimum(current_row[1:], current_row[0:-1] + 1)\n",
    "            previous_row = current_row\n",
    "        return(previous_row[-1] / len(source))\n",
    "\n",
    "# Defining a function to calculate word error rate\n",
    "def wer(recognised, actual):\n",
    "    recognised = recognised.split()\n",
    "    actual = actual.split()\n",
    "    d = np.zeros(((len(recognised) + 1), (len(actual) + 1)), dtype = int)\n",
    "    for i in range(len(recognised) + 1):\n",
    "        for j in range(len(actual) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "    for i in range(1, len(recognised) + 1):\n",
    "        for j in range(1, len(actual) + 1):\n",
    "            if recognised[i - 1] == actual[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return(d[len(recognised)][len(actual)] / len(actual))\n",
    "\n",
    "# Defining a function to add perturbation to audio files\n",
    "def perturbating(true_audio_file, false_audio_file):\n",
    "    sound_1 = AS.from_file(true_audio_file)\n",
    "    sound_2 = AS.from_file(false_audio_file)\n",
    "    combined = sound_1.overlay(sound_2)\n",
    "    combined.export(true_audio_file[:(len(true_audio_file) - 4)] + \"+\" + false_audio_file[:(len(false_audio_file) - 4)] + \".wav\", format = 'wav')   \n",
    "\n",
    "# Defining a function to read audios for speech recognition\n",
    "def read(audio_file):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = r.record(source)\n",
    "    return(audio)\n",
    "\n",
    "# Defining a function to detect speech by Google\n",
    "def google_recognise(recorded_audio):\n",
    "    r = sr.Recognizer()\n",
    "    try:\n",
    "        google = r.recognize_google(recorded_audio)\n",
    "    except (sr.UnknownValueError, sr.RequestError):\n",
    "        google = \"\"\n",
    "    return(google)\n",
    "\n",
    "# Defining a function to detect speech by Wit\n",
    "def wit_recognise(recorded_audio):\n",
    "    r = sr.Recognizer()\n",
    "    try:\n",
    "        wit = r.recognize_wit(recorded_audio, 'TRQFH3GKSWAR7RUW3MNZMTETZQXS5OL2')\n",
    "    except (sr.UnknownValueError, sr.RequestError):\n",
    "        wit = \"\"\n",
    "    return(wit)\n",
    "\n",
    "# Defining a function to comapre the two methods\n",
    "def speech_recognition_api_comparison(correct_audio_file, reference, perturbed_audio_files = [], print_outputs = False, ignore_case_punctuation_marks = True):\n",
    "    audios = [correct_audio_file]\n",
    "    for perturbation in perturbed_audio_files:\n",
    "        perturbating(correct_audio_file, perturbation)\n",
    "        audios.append(correct_audio_file[:(len(correct_audio_file) - 4)] + \"+\" + perturbation[:(len(perturbation) - 4)] + \".wav\")\n",
    "    read_audios = [read(audio) for audio in audios]\n",
    "    google_recognised_texts = [google_recognise(records) for records in read_audios]\n",
    "    wit_recognised_texts = [wit_recognise(records) for records in read_audios]\n",
    "    if print_outputs == True:\n",
    "        print(\"\\nActual: \", reference, \"\\nGoogle: \", google_recognised_texts, \"\\nWit:\\t\", wit_recognised_texts, \"\\n\")\n",
    "    if ignore_case_punctuation_marks == True:\n",
    "        reference = compromise(reference)\n",
    "        google = [compromise(texts) for texts in google_recognised_texts]\n",
    "        wit = [compromise(texts) for texts in wit_recognised_texts]\n",
    "    google_levenshtein = [levenshtein(recognitions, reference) for recognitions in google_recognised_texts]\n",
    "    wit_levenshtein = [levenshtein(recognitions, reference) for recognitions in wit_recognised_texts]\n",
    "    google_wer = [wer(recognitions, reference) for recognitions in google_recognised_texts]\n",
    "    wit_wer = [wer(recognitions, reference) for recognitions in wit_recognised_texts]\n",
    "    return(google_levenshtein + wit_levenshtein + google_wer + wit_wer)\n",
    "\n",
    "# Reading the two databases\n",
    "audio_files_1 = pd.read_csv('rhys_mcg.csv', header = None)\n",
    "n_1 = len(audio_files_1)\n",
    "filename_1 = audio_files_1.iloc[:, 0]\n",
    "text_1 = audio_files_1.iloc[:, 1]\n",
    "audio_files_2 = pd.read_csv('speech_to_text_benchmark.csv', header = None)\n",
    "n_2 = len(audio_files_2)\n",
    "filename_2 = audio_files_2.iloc[:, 0]\n",
    "text_2 = audio_files_2.iloc[:, 1]\n",
    "\n",
    "# Study of first database without perturbation\n",
    "# Sampling k1 files from n1 files\n",
    "k_1 = 30\n",
    "indices_1 = list(range(n_1))\n",
    "random_indices_1 = random.sample(indices_1, k_1)\n",
    "\n",
    "# Comparing the APIs for the sampled files\n",
    "results_1 = np.empty([k_1, 4])\n",
    "for counter, random_index in enumerate(random_indices_1):\n",
    "    results_1[counter] = speech_recognition_api_comparison((str(filename_1[random_index]) + \".wav\"), text_1[random_index])\n",
    "    time.sleep(3)\n",
    "\n",
    "# Test of Significance\n",
    "average_errors_1 = np.mean(results_1, 0)\n",
    "sd_errors_1 = np.std(results_1, 0)\n",
    "t_statistics_1 = (average_errors_1 - 0) / (sd_errors_1 / (k_1 ** 0.5))\n",
    "decisions_1 = [\"Reject\" if t_stat > t.isf(0.05, (k_1 - 1)) else \"Accept\" for t_stat in t_statistics_1]\n",
    "\n",
    "# Results for the Sampled Files\n",
    "print(\"\\nRandom Indices: \", random_indices_1)\n",
    "print(\"\\nAverage Errors: \", np.around(average_errors_1, 4))\n",
    "print(\"\\nDecisions:\\t\", decisions_1)\n",
    "\n",
    "# Study of second database with perturbation\n",
    "# Perturbations to be used\n",
    "perturbation = [\"doing_the_dishes.wav\", \"exercise_bike.wav\", \"pink_noise.wav\", \"running_tap.wav\",\"white_noise.wav\"]\n",
    "\n",
    "# Sampling k2 files from n2 files\n",
    "k_2 = 20\n",
    "indices_2 = list(range(n_2))\n",
    "random_indices_2 = random.sample(indices_2, k_2)\n",
    "\n",
    "# Comparing the APIs for the sampled files\n",
    "results_2 = np.empty([k_2, (4 * (len(perturbation) + 1))])\n",
    "for counter, random_index in enumerate(random_indices_2):\n",
    "    results_2[counter] = speech_recognition_api_comparison(filename_2[random_index], text_2[random_index], perturbation)\n",
    "    time.sleep(3)\n",
    "\n",
    "# Test of Significance\n",
    "average_errors_2 = np.mean(results_2, 0)\n",
    "sd_errors_2 = np.std(results_2, 0)\n",
    "t_statistics_2 = (average_errors_2 - 0) / (sd_errors_2 / (k_2 ** 0.5))\n",
    "decisions_2 = [\"Reject\" if t_stat > t.isf(0.05, (k_2 - 1)) else \"Accept\" for t_stat in t_statistics_2]\n",
    "\n",
    "# Results for the Sampled Files\n",
    "print(\"\\nRandom Indices: \", random_indices_2)\n",
    "print(\"\\nAverage Errors: \\n\", average_errors_2)\n",
    "print(\"\\nDecisions:\\t\", decisions_2)\n",
    "\n",
    "# Machine Translation <=> text to text\n",
    "# Defining a function to translate by Google\n",
    "def google_translate(english_text):\n",
    "    return(Translator().translate(english_text, src = 'en', dest = 'fr').text)\n",
    "\n",
    "# Defining a function to translate by Yandex\n",
    "def yandex_translate(english_text):\n",
    "    return(YandexTranslate('trnsl.1.1.20180604T085058Z.6a9b155f56ef80dd.f86700fd9d7791ba171d3bf5640be786c71244a4').translate(english_text, 'en-fr')['text'][0])\n",
    "\n",
    "# Defining a function to compare the two methods\n",
    "def machine_translation_api_comparison(english_sentence, french_sentence):\n",
    "    google = google_translate(english_sentence)\n",
    "    yandex = yandex_translate(english_sentence)\n",
    "    google_vector = TfidfVectorizer().fit_transform([google, french_sentence])\n",
    "    yandex_vector = TfidfVectorizer().fit_transform([yandex, french_sentence])\n",
    "    google_cosine_similarity = cosine_similarity(google_vector[0,], google_vector[1,])\n",
    "    yandex_cosine_similarity = cosine_similarity(yandex_vector[0,], yandex_vector[1,])\n",
    "    return([google_cosine_similarity, yandex_cosine_similarity])\n",
    "\n",
    "# Defining a function to load a document into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode = 'rt', encoding = 'utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return(text)\n",
    "\n",
    "# Defining a function to split a loaded document into sentences\n",
    "def to_sentences(doc):\n",
    "    return(doc.strip().split('\\n'))\n",
    "\n",
    "# Defining a function to clean a list of lines\n",
    "def clean_lines(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in lines:\n",
    "        # normalize unicode characters\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "        # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    return(cleaned)\n",
    "\n",
    "# Reading the two ccorpora\n",
    "english_corpus = 'europarl-v7.fr-en.en'\n",
    "english_document = load_doc(english_corpus)\n",
    "english_sentences = to_sentences(english_document)\n",
    "english_sentences = clean_lines(english_sentences)\n",
    "french_corpus = 'europarl-v7.fr-en.fr'\n",
    "french_document = load_doc(french_corpus)\n",
    "french_sentences = to_sentences(french_document)\n",
    "french_sentences = clean_lines(french_sentences)\n",
    "n_3 = len(english_sentences)\n",
    "\n",
    "# Sampling k3 files from n3 files\n",
    "k_3 = 50\n",
    "indices_3 = list(range(n_3))\n",
    "random_indices_3 = random.sample(indices_3, k_3)\n",
    "\n",
    "# Comparing the APIs for the sampled files\n",
    "results_3 = np.empty([k_3, 2])\n",
    "for counter, random_index in enumerate(random_indices_3):\n",
    "    results_3[counter] = machine_translation_api_comparison(english_sentences[random_index], french_sentences[random_index])\n",
    "    time.sleep(3)\n",
    "\n",
    "# Test of Significance\n",
    "average_cosine_similarity = np.mean(results_3, 0)\n",
    "sd_cosine_similarity = np.std(results_3, 0)\n",
    "t_statistics_3 = (average_cosine_similarity - 1) / (sd_cosine_similarity / (k_3 ** 0.5))\n",
    "decisions_3 = [\"Reject\" if t_stat < t.isf(0.95, (k_3 - 1)) else \"Accept\" for t_stat in t_statistics_3]\n",
    "\n",
    "# Results for the Sampled Files\n",
    "print(\"\\nRandom Indices: \", random_indices_3)\n",
    "print(\"\\nAverage Cosine Similarities: \", np.around(average_cosine_similarity, 4))\n",
    "print(\"\\nDecisions:\\t\", decisions_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
