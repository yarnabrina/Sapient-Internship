{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniray\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Packages\n",
    "import keras, librosa, numpy as np, os\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take input of the Folder Path and returning a tuple containing Labels, Indices of the labels and one-hot encoded labels\n",
    "def get_labels(path):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)\n",
    "\n",
    "# Function to convert .wav files to MFCC\n",
    "def wav2mfcc(file_path, max_len = 32):\n",
    "    wave, _ = librosa.load(file_path, mono = True, sr = None)\n",
    "    mfcc = librosa.feature.mfcc(wave, sr = 16000)\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width = ((0, 0), (0, pad_width)), mode = 'constant')\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc\n",
    "\n",
    "# Function to save the MFCC in arrays\n",
    "def save_data_to_array(path, max_len = 32):\n",
    "    labels, _, _ = get_labels(path)\n",
    "    for label in labels:\n",
    "        # Init mfcc vectors\n",
    "        mfcc_vectors = []\n",
    "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            mfcc = wav2mfcc(wavfile, max_len = max_len)\n",
    "            mfcc_vectors.append(mfcc)\n",
    "        np.save(label + '.npy', mfcc_vectors)\n",
    "\n",
    "# Function to create a CNN model\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size = (2, 2), activation = 'relu', input_shape = (feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(64, kernel_size = (2, 2), activation = 'relu'))\n",
    "    model.add(Conv2D(128, kernel_size = (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.125))\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to split the data in subsets for training and testing\n",
    "def get_train_test(path, split_ratio = 0.8):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels(path)\n",
    "    # Getting first arrays\n",
    "    X = np.load(labels[0] + '.npy')\n",
    "    y = np.zeros(X.shape[0])\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(label + '.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value = (i + 1)))\n",
    "    assert X.shape[0] == len(y)\n",
    "    return train_test_split(X, y, test_size = (1 - split_ratio), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vectors of label - 'correct eight': 100%|████████████████████████████████████| 150/150 [00:00<00:00, 178.31it/s]\n",
      "Saving vectors of label - 'correct five': 100%|█████████████████████████████████████| 150/150 [00:00<00:00, 184.81it/s]\n",
      "Saving vectors of label - 'correct four': 100%|█████████████████████████████████████| 150/150 [00:00<00:00, 184.81it/s]\n",
      "Saving vectors of label - 'correct nine': 100%|█████████████████████████████████████| 150/150 [00:00<00:00, 178.86it/s]\n",
      "Saving vectors of label - 'correct one': 100%|██████████████████████████████████████| 150/150 [00:00<00:00, 185.73it/s]\n",
      "Saving vectors of label - 'correct seven': 100%|████████████████████████████████████| 150/150 [00:00<00:00, 183.68it/s]\n",
      "Saving vectors of label - 'correct six': 100%|██████████████████████████████████████| 150/150 [00:00<00:00, 183.68it/s]\n",
      "Saving vectors of label - 'correct three': 100%|████████████████████████████████████| 150/150 [00:00<00:00, 180.37it/s]\n",
      "Saving vectors of label - 'correct two': 100%|██████████████████████████████████████| 150/150 [00:00<00:00, 180.15it/s]\n",
      "Saving vectors of label - 'correct zero': 100%|█████████████████████████████████████| 150/150 [00:00<00:00, 177.57it/s]\n",
      "Saving vectors of label - 'perturbed eight': 100%|██████████████████████████████████| 750/750 [00:04<00:00, 157.05it/s]\n",
      "Saving vectors of label - 'perturbed five': 100%|███████████████████████████████████| 750/750 [00:04<00:00, 164.90it/s]\n",
      "Saving vectors of label - 'perturbed four': 100%|███████████████████████████████████| 750/750 [00:04<00:00, 170.06it/s]\n",
      "Saving vectors of label - 'perturbed nine': 100%|███████████████████████████████████| 750/750 [00:04<00:00, 156.36it/s]\n",
      "Saving vectors of label - 'perturbed one': 100%|████████████████████████████████████| 750/750 [00:04<00:00, 166.77it/s]\n",
      "Saving vectors of label - 'perturbed seven': 100%|██████████████████████████████████| 750/750 [00:04<00:00, 168.48it/s]\n",
      "Saving vectors of label - 'perturbed six': 100%|████████████████████████████████████| 750/750 [00:04<00:00, 170.06it/s]\n",
      "Saving vectors of label - 'perturbed three': 100%|██████████████████████████████████| 750/750 [00:04<00:00, 170.85it/s]\n",
      "Saving vectors of label - 'perturbed two': 100%|████████████████████████████████████| 750/750 [00:04<00:00, 171.71it/s]\n",
      "Saving vectors of label - 'perturbed zero': 100%|███████████████████████████████████| 750/750 [00:04<00:00, 163.38it/s]\n",
      "Saving vectors of label - 'mixed eight': 100%|██████████████████████████████████████| 900/900 [00:05<00:00, 174.55it/s]\n",
      "Saving vectors of label - 'mixed five': 100%|███████████████████████████████████████| 900/900 [00:05<00:00, 173.98it/s]\n",
      "Saving vectors of label - 'mixed four': 100%|███████████████████████████████████████| 900/900 [00:05<00:00, 179.27it/s]\n",
      "Saving vectors of label - 'mixed nine': 100%|███████████████████████████████████████| 900/900 [00:05<00:00, 176.29it/s]\n",
      "Saving vectors of label - 'mixed one': 100%|████████████████████████████████████████| 900/900 [00:04<00:00, 183.19it/s]\n",
      "Saving vectors of label - 'mixed seven': 100%|██████████████████████████████████████| 900/900 [00:05<00:00, 179.00it/s]\n",
      "Saving vectors of label - 'mixed six': 100%|████████████████████████████████████████| 900/900 [00:05<00:00, 178.96it/s]\n",
      "Saving vectors of label - 'mixed three': 100%|██████████████████████████████████████| 900/900 [00:04<00:00, 181.39it/s]\n",
      "Saving vectors of label - 'mixed two': 100%|████████████████████████████████████████| 900/900 [00:05<00:00, 174.02it/s]\n",
      "Saving vectors of label - 'mixed zero': 100%|███████████████████████████████████████| 900/900 [00:05<00:00, 163.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fixing some global parameters for all situations of digit classification\n",
    "# Number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# Specifying feature dimensions for CNN model\n",
    "feature_dim_1 = 20\n",
    "feature_dim_2 = 32\n",
    "channel = 1\n",
    "\n",
    "# Setting callbacks\n",
    "checkpoint_correct = ModelCheckpoint(filepath = 'model_correct.h5', save_best_only = True, verbose = 1)\n",
    "checkpoint_perturbed = ModelCheckpoint(filepath = 'model_perturbed.h5', save_best_only = True, verbose = 1)\n",
    "checkpoint_mixed = ModelCheckpoint(filepath = 'model_mixed.h5', save_best_only = True, verbose = 1)\n",
    "earlystop = EarlyStopping(min_delta = 0.001, patience = 5, verbose = 1)\n",
    "\n",
    "# Save modified data to array file first\n",
    "save_data_to_array(path = \"./correct data/\")\n",
    "save_data_to_array(path = \"./perturbed data/\")\n",
    "save_data_to_array(path = \"./mixed data/\")\n",
    "\n",
    "# Loading dataset\n",
    "X_train_correct, X_test_correct, y_train_correct, y_test_correct = get_train_test(\"./correct data/\")\n",
    "X_train_perturbed, X_test_perturbed, y_train_perturbed, y_test_perturbed = get_train_test(\"./perturbed data/\")\n",
    "X_train_mixed, X_test_mixed, y_train_mixed, y_test_mixed = get_train_test(\"./mixed data/\")\n",
    "\n",
    "# Performing one hot encoding\n",
    "y_train_correct_hot = to_categorical(y_train_correct)\n",
    "y_test_correct_hot = to_categorical(y_test_correct)\n",
    "y_train_perturbed_hot = to_categorical(y_train_perturbed)\n",
    "y_test_perturbed_hot = to_categorical(y_test_perturbed)\n",
    "y_train_mixed_hot = to_categorical(y_train_mixed)\n",
    "y_test_mixed_hot = to_categorical(y_test_mixed)\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X_train_correct_reshaped = X_train_correct.reshape(X_train_correct.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test_correct_reshaped = X_test_correct.reshape(X_test_correct.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_train_perturbed_reshaped = X_train_perturbed.reshape(X_train_perturbed.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test_perturbed_reshaped = X_test_perturbed.reshape(X_test_perturbed.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_train_mixed_reshaped = X_train_mixed.reshape(X_train_mixed.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test_mixed_reshaped = X_test_mixed.reshape(X_test_mixed.shape[0], feature_dim_1, feature_dim_2, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.91944, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.91944 to 1.37791, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.37791 to 0.72662, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.72662 to 0.48506, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48506 to 0.30126, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30126 to 0.21960, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.21960\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.21960\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.21960\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.21960 to 0.17739, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.17739\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.17739 to 0.14422, saving model to model_correct.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14422\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.14422\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14422\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.14422\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14422\n",
      "Epoch 00017: early stopping\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.89000, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.89000 to 0.59776, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59776 to 0.49265, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49265 to 0.44064, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44064 to 0.43816, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.43816 to 0.38138, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.38138 to 0.36781, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.36781 to 0.35718, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35718 to 0.34202, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.34202 to 0.33288, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.33288\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.33288\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.33288\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.33288 to 0.31132, saving model to model_perturbed.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31132\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31132\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31132\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31132\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31132\n",
      "Epoch 00019: early stopping\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93903, saving model to model_mixed.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93903 to 0.67555, saving model to model_mixed.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67555 to 0.48314, saving model to model_mixed.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48314 to 0.42850, saving model to model_mixed.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42850 to 0.37280, saving model to model_mixed.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.37280 to 0.36930, saving model to model_mixed.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.36930 to 0.32847, saving model to model_mixed.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.32847\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.32847\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32847\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32847\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32847\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Defining CNN models\n",
    "model_correct = get_model()\n",
    "model_perturbed = get_model()\n",
    "model_mixed = get_model()\n",
    "\n",
    "# Training the models\n",
    "model_correct.fit(X_train_correct_reshaped, y_train_correct_hot, batch_size = 100, epochs = 100, validation_split = 0.25, callbacks = [checkpoint_correct, earlystop], verbose = 0)\n",
    "model_perturbed.fit(X_train_perturbed_reshaped, y_train_perturbed_hot, batch_size = 100, epochs = 100, validation_split = 0.25, callbacks = [checkpoint_perturbed, earlystop], verbose = 0)\n",
    "model_mixed.fit(X_train_mixed_reshaped, y_train_mixed_hot, batch_size = 100, epochs = 100, validation_split = 0.25, callbacks = [checkpoint_mixed, earlystop], verbose = 0)\n",
    "\n",
    "# Evaluating the models\n",
    "model_correct = load_model('model_correct.h5')\n",
    "model_perturbed = load_model('model_perturbed.h5')\n",
    "model_mixed = load_model('model_mixed.h5')\n",
    "correct_correct_scores = model_correct.evaluate(X_test_correct_reshaped, y_test_correct_hot, verbose = 0)\n",
    "correct_perturbed_scores = model_correct.evaluate(X_test_perturbed_reshaped, y_test_perturbed_hot, verbose = 0)\n",
    "correct_mixed_scores = model_correct.evaluate(X_test_mixed_reshaped, y_test_mixed_hot, verbose = 0)\n",
    "perturbed_correct_scores = model_perturbed.evaluate(X_test_correct_reshaped, y_test_correct_hot, verbose = 0)\n",
    "perturbed_perturbed_scores = model_perturbed.evaluate(X_test_perturbed_reshaped, y_test_perturbed_hot, verbose = 0)\n",
    "perturbed_mixed_scores = model_perturbed.evaluate(X_test_mixed_reshaped, y_test_mixed_hot, verbose = 0)\n",
    "mixed_correct_scores = model_mixed.evaluate(X_test_correct_reshaped, y_test_correct_hot, verbose = 0)\n",
    "mixed_perturbed_scores = model_mixed.evaluate(X_test_perturbed_reshaped, y_test_perturbed_hot, verbose = 0)\n",
    "mixed_mixed_scores = model_mixed.evaluate(X_test_mixed_reshaped, y_test_mixed_hot, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unperturbed Model performance on Unperturbed Data : 97.33%\n",
      "Unperturbed Model performance on Perturbed Data : 7.07%\n",
      "Unperturbed Model performance on Mixed Data : 22.11%\n",
      "Perturbed Model performance on Unperturbed Data : 6.00%\n",
      "Perturbed Model performance on Perturbed Data : 89.33%\n",
      "Perturbed Model performance on Mixed Data : 78.39%\n",
      "Mixed Model performance on Unperturbed Data : 97.67%\n",
      "Mixed Model performance on Perturbed Data : 90.67%\n",
      "Mixed Model performance on Mixed Data : 88.50%\n"
     ]
    }
   ],
   "source": [
    "# Printing results\n",
    "print(\"Unperturbed Model performance on Unperturbed Data : %.2f%%\" % (correct_correct_scores[1] * 100))\n",
    "print(\"Unperturbed Model performance on Perturbed Data : %.2f%%\" % (correct_perturbed_scores[1] * 100))\n",
    "print(\"Unperturbed Model performance on Mixed Data : %.2f%%\" % (correct_mixed_scores[1] * 100))\n",
    "print(\"Perturbed Model performance on Unperturbed Data : %.2f%%\" % (perturbed_correct_scores[1] * 100))\n",
    "print(\"Perturbed Model performance on Perturbed Data : %.2f%%\" % (perturbed_perturbed_scores[1] * 100))\n",
    "print(\"Perturbed Model performance on Mixed Data : %.2f%%\" % (perturbed_mixed_scores[1] * 100))\n",
    "print(\"Mixed Model performance on Unperturbed Data : %.2f%%\" % (mixed_correct_scores[1] * 100))\n",
    "print(\"Mixed Model performance on Perturbed Data : %.2f%%\" % (mixed_perturbed_scores[1] * 100))\n",
    "print(\"Mixed Model performance on Mixed Data : %.2f%%\" % (mixed_mixed_scores[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
